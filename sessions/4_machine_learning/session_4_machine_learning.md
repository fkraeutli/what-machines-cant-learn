[Home](../../README.md) / [Sessions](../README.md) / Session 4: Machine Learning

# Session 4: Machine Learning

* [Lecture](#lecture)
* [Reading](#reading)
* [Hands-On Exercise](#hands-on-exercise)
	* [Files](#files)

## Lecture

### History of AI Continued

> Slides: [History of AI continued](lecture/slides_history_of_ai_continued.md)

Continuing our introduction to the history of artificial intelligence the lecture explores the connectionist approach to AI. Statistical methods in the form of Neural Networks and implemented in Rosenblatt's Perceptron as early as 1958 didn't quite catch on until the mid 1980's. Not only due to the obstacles outlined in the last lecture, but also due to the unavailability of suitable training data, sufficient computing power and, last but definitely not least, the absence of a suitable algorithm to train multi-layer neural networks, which are required to solve problems that are not linearly separable.

Despite the publication of the [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation) algorithm for training multi-layer neural networks in 1986, the dominant AI methods until 2015 used other approaches. [IBM Deep Blue](https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)), the AI that beat [Garry Kasparov](https://en.wikipedia.org/wiki/Garry_Kasparov) in chess, ran a search algorithm that calculates every possible sequence of moves and counter-moves, effectively taking a brute-force approach to winning at chess enabled by increasing computational power.

[Deep Learning](https://en.wikipedia.org/wiki/Deep_learning), the arguably most popular AI method at the time of writing, took off after a Deep Neural Network outperformed all previously available techniques in the 2015 [ImageNet Challenge](http://www.image-net.org/challenges/LSVRC/). 

### Introduction to Neural Networks

## Reading

## Hands-On Exercise

### Files