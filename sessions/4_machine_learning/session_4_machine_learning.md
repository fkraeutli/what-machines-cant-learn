[Home](../../README.md) / [Sessions](../README.md) / Session 4: Machine Learning

# Session 4: Machine Learning

* [Lecture](#lecture)
* [Reading](#reading)
* [Hands-On Exercise](#hands-on-exercise)
	* [Files](#files)

## Lecture

### History of AI Continued

> Slides: [History of AI continued](lecture/slides_history_of_ai_continued.md)

Continuing our introduction to the history of artificial intelligence the lecture explores the connectionist approach to AI. Statistical methods in the form of Neural Networks and implemented in Rosenblatt's Perceptron as early as 1958 didn't quite catch on until the mid 1980's. Not only due to the obstacles outlined in the last lecture, but also due to the unavailability of suitable training data, sufficient computing power and, last but definitely not least, the absence of a suitable algorithm to train multi-layer neural networks, which are required to solve problems that are not linearly separable.

Despite the publication of the [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation) algorithm for training multi-layer neural networks in 1986, the dominant AI methods until 2015 used other approaches. [IBM Deep Blue](https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)), the AI that beat [Garry Kasparov](https://en.wikipedia.org/wiki/Garry_Kasparov) in chess, ran a search algorithm that calculates every possible sequence of moves and counter-moves, effectively taking a brute-force approach to winning at chess enabled by increasing computational power.

[Deep Learning](https://en.wikipedia.org/wiki/Deep_learning), the arguably most popular AI method at the time of writing, took off after a Deep Neural Network outperformed all previously available techniques in the 2015 [ImageNet Challenge](http://www.image-net.org/challenges/LSVRC/). 

### Introduction to Neural Networks

The second lecture outlines the mechanisms by which a neural network functions. Rather than employing the metaphor of neurons and the brain, neural networks are presented as a type of [computational graph](lecture/notes_3_functions_as_computational_graphs.md)) and a function for making predictions based on data.

Students are introduced to (linear) functions and how they can be used to make predictions based on the technique of [linear regression](https://en.wikipedia.org/wiki/Linear_regression). Computational graphs are then introduced and the mechanisms of how an arbitrary computational graph can be used to model data is demonstrated using an [interactive tool]((lecture/exercise_1_computational_graph.md)).

Finally, the distinct features of neural networks are outlined and the importance of the [activation function](https://en.wikipedia.org/wiki/Activation_function) for training them.

The lecture uses a whiteboard and an [online exercise](lecture/exercise_1_computational_graph.md). The sketches and instructions are reproduced here:

* [Introduction](lecture/notes_0_introduction_to_neural_networks.md)
* [Linear Functions](lecture/notes_1_linear_functions.md)
* [Modelling Data with Linear Functions](lecture/notes_2_modelling_data_with_linear_functions.md)
* [Representing Functions as Computational Graphs](lecture/notes_3_functions_as_computational_graphs.md)
* [Training a Computational Graph on Data](lecture/notes_4_training_a_computational_graph.md)
* [Computational Graph Exercise](lecture/exercise_1_computational_graph.md)
* [Neural Networks](lecture/notes_5_neural_networks.md)


## Reading

## Hands-On Exercise

### Files